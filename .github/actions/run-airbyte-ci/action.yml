name: "Run Dagger pipeline"
description: "Runs a given dagger pipeline"
inputs:
  subcommand:
    description: "Subcommand for airbyte-ci"
    required: true
  context:
    description: "CI context (e.g., pull_request, manual)"
    required: true
  github_token:
    description: "GitHub token"
    required: false
  dagger_cloud_token:
    description: "Dagger Cloud token"
    required: false
  docker_hub_username:
    description: "Dockerhub username"
    required: false
  docker_hub_password:
    description: "Dockerhub password"
    required: false
  options:
    description: "Options for the subcommand"
    required: false
  production:
    description: "Whether to run in production mode"
    required: false
    default: "True"
  report_bucket_name:
    description: "Bucket name for CI reports"
    required: false
    default: "airbyte-ci-reports-multi"
  gcp_gsm_credentials:
    description: "GCP credentials for GCP Secret Manager"
    required: false
    default: ""
  gcp_integration_tester_credentials:
    description: "GCP credentials for integration tests"
    required: false
    default: ""
  git_repo_url:
    description: "Git repository URL"
    default: https://github.com/airbytehq/airbyte.git
    required: false
  git_branch:
    description: "Git branch to checkout"
    required: false
  git_revision:
    description: "Git revision to checkout"
    required: false
  slack_webhook_url:
    description: "Slack webhook URL"
    required: false
  metadata_service_gcs_credentials:
    description: "GCP credentials for metadata service"
    required: false
  metadata_service_bucket_name:
    description: "Bucket name for metadata service"
    required: false
    default: "prod-airbyte-cloud-connector-metadata-service"
  sentry_dsn:
    description: "Sentry DSN"
    required: false
  spec_cache_bucket_name:
    description: "Bucket name for GCS spec cache"
    required: false
    default: "io-airbyte-cloud-spec-cache"
  spec_cache_gcs_credentials:
    description: "GCP credentials for GCS spec cache"
    required: false
  gcs_credentials:
    description: "GCP credentials for GCS"
    required: false
  ci_job_key:
    description: "CI job key"
    required: false
  s3_build_cache_access_key_id:
    description: "Gradle S3 Build Cache AWS access key ID"
    required: false
  s3_build_cache_secret_key:
    description: "Gradle S3 Build Cache AWS secret key"
    required: false
  airbyte_ci_binary_url:
    description: "URL to airbyte-ci binary"
    required: false
    default: https://connectors.airbyte.com/airbyte-ci/releases/ubuntu/latest/airbyte-ci
  python_registry_token:
    description: "Python registry API token to publish python package"
    required: false
  is_fork:
    description: "Whether the PR is from a fork"
    required: false
    default: "false"
  max_attempts:
    description: "Number of attempts at running the airbyte-ci command"
    required: false
    default: 1
  retry_wait_seconds:
    description: "Number of seconds to wait between retry attempts"
    required: false
    default: 60

runs:
  using: "composite"
  steps:
    # - name: Get start timestamp
    #   id: get-start-timestamp
    #   shell: bash
    #   run: echo "start-timestamp=$(date +%s)" >> $GITHUB_OUTPUT
    # - name: Debug-print local paths checked out
    #   shell: bash
    #   run: |
    #     set -x
    #     echo "Working directory: $(pwd)"
    #     ls -la
    #     ls -la airbyte-python-cdk || echo "No airbyte-python-cdk directory"
    #     ls -laL ../airbyte-python-cdk || echo "No airbyte-python-cdk symlink"
    - name: Install Java Environment
      id: install-java-environment
      uses: ./.github/actions/install-java-environment
    # - name: Docker login
    #   id: docker-login
    #   uses: docker/login-action@v3
    #   if: ${{ inputs.docker_hub_username != '' && inputs.docker_hub_password != '' }}
    #   with:
    #     username: ${{ inputs.docker_hub_username }}
    #     password: ${{ inputs.docker_hub_password }}
    - name: Install Airbyte CI
      id: install-airbyte-ci
      uses: ./.github/actions/install-airbyte-ci
      with:
        airbyte_ci_binary_url: ${{ inputs.airbyte_ci_binary_url }}
        is_fork: ${{ inputs.is_fork }}
    - name: Run airbyte-ci
      id: run-airbyte-ci
      shell: bash
      env:
        CI: "False"
        # Next environment variables are workflow inputs based and can be set with empty values if the inputs are not required and passed
        CI_CONTEXT: "${{ inputs.context }}"
      run: |
        export PATH="$PATH:/root/.local/bin"
        airbyte-ci --disable-update-check --disable-dagger-run --is-local ${{ inputs.subcommand }}
    - name: Stop Engine
      id: stop-engine
      if: always()
      shell: bash
      run: |
        mapfile -t containers < <(docker ps --filter name="dagger-engine-*" -q)
        if [[ "${#containers[@]}" -gt 0 ]]; then
          # give 5mn to the Dagger Engine to push cache data to Dagger Cloud
          docker stop -t 300 "${containers[@]}";
        fi

    # - name: Collect dagger engine logs
    #   id: collect-dagger-engine-logs
    #   if: always()
    #   uses: jwalton/gh-docker-logs@v2
    #   with:
    #     dest: "./dagger_engine_logs"
    #     images: "registry.dagger.io/engine"

    # - name: Tar logs
    #   id: tar-logs
    #   if: always()
    #   shell: bash
    #   run: tar cvzf ./dagger_engine_logs.tgz ./dagger_engine_logs

    # - name: Hash subcommand
    #   id: hash-subcommand
    #   shell: bash
    #   if: always()
    #   run: echo "subcommand_hash=$(echo ${{ inputs.subcommand }} | sha256sum | cut -d ' ' -f 1)" >> $GITHUB_OUTPUT

    # - name: Upload logs to GitHub
    #   id: upload-dagger-engine-logs
    #   if: always()
    #   uses: actions/upload-artifact@v4
    #   with:
    #     name: ${{ github.job }}_${{ steps.hash-subcommand.outputs.subcommand_hash }}_dagger_engine_logs.tgz
    #     path: ./dagger_engine_logs.tgz
    #     retention-days: 7
